# -*- coding: utf-8 -*-
"""logistic_regression_assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/178FKMWGH-hy59-UjmZUHR9CRkVl1slQT
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, roc_auc_score

train_df = pd.read_csv("Titanic_train.csv")
test_df = pd.read_csv("Titanic_test.csv")

train_df['Age'].fillna(train_df['Age'].median(), inplace=True)
train_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace=True)
train_df['Cabin'].fillna('Unknown', inplace=True)
train_df['Fare'].fillna(train_df['Fare'].median(), inplace=True)

# Encode categorical features

le = LabelEncoder()
for col in ['Sex', 'Embarked']:
    train_df[col] = le.fit_transform(train_df[col])

train_df.hist(figsize=(10, 8))
plt.tight_layout()
plt.show()

sns.pairplot(train_df)
plt.show()

sns.boxplot(x='Survived', y='Age', data=train_df)
plt.show()

# Split features and target

X = train_df.drop(['Survived', 'Name', 'Ticket', 'Cabin'], axis=1)
y = train_df['Survived']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#  Train model

model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]

print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# ROC CURVE
fpr, tpr, _ = roc_curve(y_test, y_prob)
auc = roc_auc_score(y_test, y_prob)

plt.plot(fpr, tpr, color='red', label=f'Logistic Regression (AUC = {auc:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()



#interpretation of logstic regression coeffcients
coef_df = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': model.coef_[0]
}).sort_values(by='Coefficient', ascending=False)

print(coef_df)



import statsmodels.api as sm

X_sm = sm.add_constant(X)  # Add intercept
model_sm = sm.Logit(y, X_sm)
result = model_sm.fit()

print(result.summary())



import joblib
joblib.dump(model, "titanic_model.pkl")



!pip install streamlit



import streamlit as st
import pandas as pd
import numpy as np
import joblib

# Load the model
model = joblib.load("titanic_model.pkl")

st.title("Titanic Survival Prediction App")

st.write("Enter passenger details below:")

# User input fields
pclass = st.selectbox("Passenger Class (1 = 1st, 2 = 2nd, 3 = 3rd)", [1, 2, 3])
sex = st.selectbox("Sex", ["male", "female"])
age = st.slider("Age", 0, 80, 25)
sibsp = st.number_input("Number of Siblings/Spouses aboard (SibSp)", 0, 8, 0)
parch = st.number_input("Number of Parents/Children aboard (Parch)", 0, 6, 0)
fare = st.number_input("Fare", 0.0, 500.0, 32.0)
embarked = st.selectbox("Port of Embarkation", ["C", "Q", "S"])

# Encode categorical variables
sex = 1 if sex == "female" else 0
embarked_map = {"C": 0, "Q": 1, "S": 2}
embarked = embarked_map[embarked]

# Create input DataFrame
input_data = pd.DataFrame({
    'PassengerId': [0],
    'Pclass': [pclass],
    'Sex': [sex],
    'Age': [age],
    'SibSp': [sibsp],
    'Parch': [parch],
    'Fare': [fare],
    'Embarked': [embarked]
})

# Drop unused columns if model was trained differently
input_data = input_data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]

# Predict
if st.button("Predict Survival"):
    prediction = model.predict(input_data)[0]
    prob = model.predict_proba(input_data)[0][1]

    if prediction == 1:
        st.success(f"üéâ Survived (Probability: {prob:.2f})")
    else:
        st.error(f"üíÄ Did not survive (Probability: {prob:.2f})")



"""1. What is the difference between precision and recall?

A.Precision tells us how many of the predicted positives are actually correct, while recall tells us how many of the actual positives were successfully detected.

2. What is cross-validation, and why is it important in binary classification?

A.‚ÄúCross-validation is a technique where the dataset is split into multiple folds to train and test the model multiple times. It‚Äôs important in binary classification because it helps evaluate how well the model generalizes, prevents overfitting, and ensures reliable performance across both classes.‚Äù
"""

